{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9f05f4",
   "metadata": {},
   "source": [
    "# Deep Learning Model for Sentiment Analysis\n",
    "\n",
    "This notebook creates and trains deep learning models for sentiment analysis on movie reviews. The notebook includes:\n",
    "- Data loading and preprocessing\n",
    "- Memory optimization for different system configurations\n",
    "- CNN and Transformer model architectures\n",
    "- Model training with optimized parameters\n",
    "- Model evaluation and prediction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0332b4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 2)\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pickle\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a2302",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('final_combined_dataset_clean.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ffa45",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed!\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    50000\n",
      "0    50000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def enhanced_preprocess_text(text):\n",
    "    \"\"\"Enhanced text preprocessing for sentiment analysis\"\"\"\n",
    "    text = text.lower()\n",
    "    # Keep important punctuation that might indicate sentiment\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?.,]', '', text)\n",
    "    # Convert multiple exclamation/question marks to single ones\n",
    "    text = re.sub(r'(!)\\1+', r'!', text)\n",
    "    text = re.sub(r'(\\?)\\1+', r'?', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying text preprocessing...\")\n",
    "df['review'] = df['review'].apply(enhanced_preprocess_text)\n",
    "\n",
    "# Convert sentiment labels to binary (0 for negative, 1 for positive)\n",
    "df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Final sentiment distribution:\\n{df['sentiment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec99893",
   "metadata": {},
   "source": [
    "## 4. Memory Optimization and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa71884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information:\n",
      "Total RAM: 13.9 GB\n",
      "Available RAM: 4.8 GB\n",
      "Python process memory: 0.6 GB\n",
      "\n",
      "Memory Estimation (approximate):\n",
      "MAX_FEATURES=10000, EMB_DIM=256: ~9.8 MB for embeddings\n",
      "MAX_FEATURES=10000, EMB_DIM=300: ~11.4 MB for embeddings\n",
      "MAX_FEATURES=15000, EMB_DIM=256: ~14.6 MB for embeddings\n",
      "MAX_FEATURES=15000, EMB_DIM=300: ~17.2 MB for embeddings\n",
      "MAX_FEATURES=20000, EMB_DIM=256: ~19.5 MB for embeddings\n",
      "MAX_FEATURES=20000, EMB_DIM=300: ~22.9 MB for embeddings\n",
      "\n",
      "Using MAX_FEATURES = 15000 (Moderate for <8GB RAM)\n",
      "Average review length: 231 tokens\n",
      "Median review length: 173 tokens\n",
      "90th percentile length: 452 tokens\n",
      "Tokenization completed!\n",
      "Vocabulary size: 145314\n",
      "X shape: (100000, 300)\n",
      "y shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "def estimate_system_resources():\n",
    "    \"\"\"Estimate system resources and recommend configuration\"\"\"\n",
    "    print(\"=== System Resource Analysis ===\")\n",
    "    print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "    print(f\"Current Python process memory: {psutil.Process().memory_info().rss / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # Memory estimation for different configurations\n",
    "    embedding_dims = [200, 256, 300]\n",
    "    max_features_options = [10000, 15000, 20000]\n",
    "    \n",
    "    print(\"\\n=== Memory Estimation for Model Configurations ===\")\n",
    "    for max_feat in max_features_options:\n",
    "        for emb_dim in embedding_dims:\n",
    "            embedding_memory = max_feat * emb_dim * 4 / (1024**2)  # 4 bytes per float32\n",
    "            print(f\"MAX_FEATURES={max_feat:,}, EMB_DIM={emb_dim}: ~{embedding_memory:.1f} MB for embeddings\")\n",
    "    \n",
    "    return psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "def get_optimal_config(available_gb):\n",
    "    \"\"\"Get optimal configuration based on available memory\"\"\"\n",
    "    if available_gb < 4:\n",
    "        config = {\n",
    "            'MAX_FEATURES': 10000,\n",
    "            'MAX_LENGTH': 250,\n",
    "            'BATCH_SIZE': 32,\n",
    "            'EPOCHS': 12,\n",
    "            'EMBEDDING_DIM': 200,\n",
    "            'description': 'Conservative (for <4GB RAM)'\n",
    "        }\n",
    "    elif available_gb < 8:\n",
    "        config = {\n",
    "            'MAX_FEATURES': 15000,\n",
    "            'MAX_LENGTH': 300,\n",
    "            'BATCH_SIZE': 64,\n",
    "            'EPOCHS': 15,\n",
    "            'EMBEDDING_DIM': 256,\n",
    "            'description': 'Moderate (for <8GB RAM)'\n",
    "        }\n",
    "    else:\n",
    "        config = {\n",
    "            'MAX_FEATURES': 20000,\n",
    "            'MAX_LENGTH': 300,\n",
    "            'BATCH_SIZE': 128,\n",
    "            'EPOCHS': 20,\n",
    "            'EMBEDDING_DIM': 300,\n",
    "            'description': 'Full (for >=8GB RAM)'\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n=== Selected Configuration: {config['description']} ===\")\n",
    "    for key, value in config.items():\n",
    "        if key != 'description':\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Analyze system and get optimal configuration\n",
    "available_gb = estimate_system_resources()\n",
    "config = get_optimal_config(available_gb)\n",
    "\n",
    "# Set global configuration variables\n",
    "MAX_FEATURES = config['MAX_FEATURES']\n",
    "MAX_LENGTH = config['MAX_LENGTH']\n",
    "BATCH_SIZE = config['BATCH_SIZE']\n",
    "EPOCHS = config['EPOCHS']\n",
    "EMBEDDING_DIM = config['EMBEDDING_DIM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a808391",
   "metadata": {},
   "source": [
    "## 5. Text Tokenization and Sequence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d27784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"=== Tokenization Process ===\")\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Analyze sequence lengths\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "print(f\"Sequence length statistics:\")\n",
    "print(f\"  Average: {np.mean(lengths):.0f} tokens\")\n",
    "print(f\"  Median: {np.median(lengths):.0f} tokens\")\n",
    "print(f\"  90th percentile: {np.percentile(lengths, 90):.0f} tokens\")\n",
    "print(f\"  95th percentile: {np.percentile(lengths, 95):.0f} tokens\")\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print(f\"\\n=== Tokenization Results ===\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index):,}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e865921",
   "metadata": {},
   "source": [
    "## 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80000\n",
      "Test set size: 20000\n"
     ]
    }
   ],
   "source": [
    "# Split the data with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"=== Data Split Results ===\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Training set positive ratio: {y_train.mean():.3f}\")\n",
    "print(f\"Test set positive ratio: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b2f64",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    \"\"\"Create CNN model optimized for current system configuration\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=MAX_FEATURES, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "        \n",
    "        # Multi-scale CNN layers\n",
    "        tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(256, 7, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_transformer_model():\n",
    "    \"\"\"Create Transformer model with multi-head attention\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(MAX_LENGTH,))\n",
    "    \n",
    "    # Embedding with positional encoding\n",
    "    embedding = Embedding(input_dim=MAX_FEATURES, output_dim=EMBEDDING_DIM)(inputs)\n",
    "    position_encoding = tf.keras.layers.Dense(EMBEDDING_DIM, activation='linear')(embedding)\n",
    "    embedding = tf.keras.layers.Add()([embedding, position_encoding])\n",
    "    \n",
    "    # Multi-head attention layers\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8, key_dim=32, dropout=0.1\n",
    "    )(embedding, embedding)\n",
    "    attention_output = tf.keras.layers.Add()([embedding, attention_output])\n",
    "    attention_output = tf.keras.layers.LayerNormalization()(attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn = tf.keras.layers.Dense(EMBEDDING_DIM * 2, activation='relu')(attention_output)\n",
    "    ffn = tf.keras.layers.Dense(EMBEDDING_DIM)(ffn)\n",
    "    ffn = tf.keras.layers.Dropout(0.1)(ffn)\n",
    "    ffn_output = tf.keras.layers.Add()([attention_output, ffn])\n",
    "    ffn_output = tf.keras.layers.LayerNormalization()(ffn_output)\n",
    "    \n",
    "    # Global pooling and classification\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(ffn_output)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(ffn_output)\n",
    "    pooled = tf.keras.layers.Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pooled)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Model architectures defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e257e0c",
   "metadata": {},
   "source": [
    "## 8. Training Configuration and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34976d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks(model_name):\n",
    "    \"\"\"Create optimized callbacks for training\"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "    \n",
    "    # Learning rate reduction\n",
    "    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(lr_schedule)\n",
    "    \n",
    "    # Model checkpointing\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'best_{model_name}_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(checkpoint)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    class MemoryCleanupCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            gc.collect()\n",
    "    \n",
    "    callbacks.append(MemoryCleanupCallback())\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(\"Training callbacks configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22a8b8",
   "metadata": {},
   "source": [
    "## 9. Model Training - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d900ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CNN model\n",
    "print(\"=== Training CNN Model ===\")\n",
    "cnn_model = create_cnn_model()\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the model\n",
    "cnn_callbacks = create_training_callbacks('cnn')\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=cnn_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"CNN training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b35e0e",
   "metadata": {},
   "source": [
    "## 10. Model Training - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Transformer model\n",
    "print(\"=== Training Transformer Model ===\")\n",
    "transformer_model = create_transformer_model()\n",
    "transformer_model.summary()\n",
    "\n",
    "# Train the model\n",
    "transformer_callbacks = create_training_callbacks('transformer')\n",
    "history_transformer = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=transformer_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Transformer training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c80c5",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd97e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, X_test, y_test):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(f\"\\n=== {model_name} Model Evaluation ===\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    test_loss, test_accuracy, test_auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Predictions and classification report\n",
    "    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int)\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    return test_accuracy, test_auc\n",
    "\n",
    "# Evaluate both models\n",
    "cnn_accuracy, cnn_auc = evaluate_model(cnn_model, \"CNN\", X_test, y_test)\n",
    "transformer_accuracy, transformer_auc = evaluate_model(transformer_model, \"Transformer\", X_test, y_test)\n",
    "\n",
    "# Model comparison\n",
    "print(f\"\\n=== Model Comparison ===\")\n",
    "print(f\"CNN - Accuracy: {cnn_accuracy:.4f}, AUC: {cnn_auc:.4f}\")\n",
    "print(f\"Transformer - Accuracy: {transformer_accuracy:.4f}, AUC: {transformer_auc:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "if cnn_accuracy > transformer_accuracy:\n",
    "    best_model = cnn_model\n",
    "    best_model_name = \"CNN\"\n",
    "    best_history = history_cnn\n",
    "else:\n",
    "    best_model = transformer_model\n",
    "    best_model_name = \"Transformer\"\n",
    "    best_history = history_transformer\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cebb6",
   "metadata": {},
   "source": [
    "## 12. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22656737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # AUC plot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['auc'], label='Training AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.title(f'{model_name} - AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(history_cnn, \"CNN\")\n",
    "plot_training_history(history_transformer, \"Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d3187",
   "metadata": {},
   "source": [
    "## 13. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model=best_model):\n",
    "    \"\"\"Predict sentiment of a new review\"\"\"\n",
    "    # Preprocess the text\n",
    "    cleaned_text = enhanced_preprocess_text(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test with sample reviews\n",
    "sample_reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and storyline.\",\n",
    "    \"Terrible movie. Boring plot and bad acting.\",\n",
    "    \"The movie was okay, nothing special but not bad either.\",\n",
    "    \"Amazing cinematography and brilliant performances by all actors!\",\n",
    "    \"Worst movie I've ever seen. Complete waste of time.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n=== Testing Sentiment Prediction ({best_model_name} Model) ===\")\n",
    "for i, review in enumerate(sample_reviews, 1):\n",
    "    sentiment, confidence = predict_sentiment(review)\n",
    "    print(f\"{i}. Review: '{review}'\")\n",
    "    print(f\"   Predicted: {sentiment} (Confidence: {confidence:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5dd27",
   "metadata": {},
   "source": [
    "## 14. Model and Configuration Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51629826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "print(\"=== Saving Models and Configuration ===\")\n",
    "\n",
    "# Save both models\n",
    "cnn_model.save('sentiment_cnn_model.keras')\n",
    "transformer_model.save('sentiment_transformer_model.keras')\n",
    "best_model.save('best_sentiment_model.keras')\n",
    "\n",
    "print(f\"✓ CNN model saved as 'sentiment_cnn_model.keras'\")\n",
    "print(f\"✓ Transformer model saved as 'sentiment_transformer_model.keras'\")\n",
    "print(f\"✓ Best model ({best_model_name}) saved as 'best_sentiment_model.keras'\")\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"✓ Tokenizer saved as 'tokenizer.pkl'\")\n",
    "\n",
    "# Save all configuration parameters\n",
    "model_config = {\n",
    "    'MAX_FEATURES': MAX_FEATURES,\n",
    "    'MAX_LENGTH': MAX_LENGTH,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'best_model_name': best_model_name,\n",
    "    'best_model_accuracy': float(cnn_accuracy if best_model_name == \"CNN\" else transformer_accuracy),\n",
    "    'system_config': config['description']\n",
    "}\n",
    "\n",
    "with open('model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "print(\"✓ Model configuration saved as 'model_config.pkl'\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Final accuracy: {model_config['best_model_accuracy']:.4f}\")\n",
    "print(f\"Configuration: {model_config['system_config']}\")\n",
    "print(\"All files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26e3b7",
   "metadata": {},
   "source": [
    "## 15. Final Notes\n",
    "\n",
    "This notebook has successfully:\n",
    "- ✅ Loaded and preprocessed the sentiment analysis dataset\n",
    "- ✅ Optimized configuration based on system resources\n",
    "- ✅ Created and trained both CNN and Transformer models\n",
    "- ✅ Evaluated model performance with comprehensive metrics\n",
    "- ✅ Implemented prediction functions for new text\n",
    "- ✅ Saved all models and configurations for future use\n",
    "\n",
    "The models are now ready for deployment and can be loaded using the saved files for making predictions on new movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
